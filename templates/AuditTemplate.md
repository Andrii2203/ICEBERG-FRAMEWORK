# Iceberg Audit Report

## 1. Scope
(What was audited)

## 2. Findings
### 2.1 Structural Issues
### 2.2 Architectural Issues
### 2.3 Code Quality Issues
### 2.4 Missing Functionality
### 2.5 Terminology Conflicts

## 3. Severity Levels
- Critical  
- High  
- Medium  
- Low  

## 4. Recommendations
(Per issue)

## 5. STOP‑CHECK
- All findings documented  
- No assumptions  
- Terminology consistent  
# Iceberg Audit Report — {{AUDIT_TYPE}}

## 1. Scope
Describe what was audited and why.

- **Audit Type:** {{AUDIT_TYPE}}
- **Target:** {{TARGET}}
- **Source Material:** {{INPUTS}}
- **Standards Used:** {{ICEBERG_STANDARDS}}
- **Constraints:** {{CONSTRAINTS}}

---

## 2. Methodology
Explain how the audit was performed.

### 2.1 Audit Method
- Static analysis  
- Visual analysis  
- Architecture review  
- Standards comparison  
- Heuristic evaluation  
- Deterministic Iceberg Protocol  

### 2.2 Evidence Rules
All findings must be:
- Verifiable  
- Reproducible  
- Based on provided material  
- Free of assumptions  

---

## 3. Findings Overview
Summary of all issues grouped by category.

### 3.1 Structural Issues
(Information architecture, folder structure, routing, layout)

### 3.2 Architectural Issues
(Layering, boundaries, data flow, SSR/CSR, domain purity)

### 3.3 Code Quality Issues
(If code exists: patterns, smells, anti‑patterns)

### 3.4 Missing Functionality
(Required but absent features, metadata, accessibility, performance)

### 3.5 Terminology Conflicts
(Inconsistent naming, domain mismatches)

### 3.6 Standards Violations
(SEO, A11y, Performance, Architecture)

---

## 4. Detailed Findings (Per Category)

### 4.1 Structural Issues
For each issue:
- **ID:** STR‑001  
- **Description:**  
- **Evidence:**  
- **Impact:**  
- **Severity:**  
- **Standard Violated:**  
- **Fix Recommendation:**  

### 4.2 Architectural Issues
Same structure as above.

### 4.3 Code Quality Issues
Same structure as above.

### 4.4 Missing Functionality
Same structure as above.

### 4.5 Terminology Conflicts
Same structure as above.

---

## 5. Severity Levels (Iceberg Standard)

| Level     | Definition |
|-----------|------------|
| **Critical** | Blocks functionality, breaks architecture, or violates core standards |
| **High**     | Major issue affecting scalability, correctness, or UX |
| **Medium**   | Noticeable issue but not blocking |
| **Low**      | Minor improvement or polish |

---

## 6. Recommendations Summary
A prioritized list of what must be fixed.

### 6.1 Critical Fixes
(Blocking issues)

### 6.2 High Priority Fixes
(Important but not blocking)

### 6.3 Medium Priority Fixes
(Should be addressed)

### 6.4 Low Priority Fixes
(Nice to have)

---

## 7. Compliance Matrix
Map findings to standards.

| Standard | Status | Notes |
|----------|--------|--------|
| Architecture | Pass/Fail | … |
| SEO | Pass/Fail | … |
| Accessibility | Pass/Fail | … |
| Performance | Pass/Fail | … |
| Content | Pass/Fail | … |

---

## 8. Risk Assessment
List risks introduced by current issues.

- **Technical Risk:**  
- **Scalability Risk:**  
- **Maintainability Risk:**  
- **SEO Risk:**  
- **Accessibility Risk:**  
- **Performance Risk:**  

---

## 9. Final Verdict
Short summary of the system’s current state.

- **Overall Quality:** {{RATING}}  
- **Compliance Level:** {{LEVEL}}  
- **Readiness for Implementation:** Yes/No  

---

## 10. STOP‑CHECK
- [ ] All findings documented  
- [ ] No assumptions  
- [ ] Evidence provided  
- [ ] Severity assigned  
- [ ] Standards mapped  
- [ ] Recommendations complete  
- [ ] Terminology consistent  
